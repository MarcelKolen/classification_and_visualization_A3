{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bbe3b15",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "## JuPyter Notebook - Verschuur L. 1811053, Kolenbrander M. 1653415\n",
    "\n",
    "Assignment 3 of Advances in Datamining has two main tasks:\n",
    "- Master classification algorithms\n",
    "- Master visualisation and dimensionality reduction algorithms\n",
    "\n",
    "Our submission consists of two notebooks and a python import file:\n",
    "- `\"AIDM Assignment 3 - Visualization.ipynb\"`\n",
    "- `\"AIDM Assignment 3 - Classification.ipynb\"`\n",
    "- `\"utility_functions.py\"`\n",
    "\n",
    "This file, `\"AIDM Assignment 3 - Classification.ipynb\"`, focusses on the classification aspect of the assignment. In the classification aspect, the assignment requires the classification, or rather prediction, of a target attribute based on a carefully selected set of input/variable attributes. In the case of this assignment, three different classification techniques are used. The classification techniques are as follows:\n",
    "- a `Random Forest` classification\n",
    "- a `Support Vector Classification` classification\n",
    "- a `XGBoost` classification\n",
    "\n",
    "The former two classification algorithms are implemented using the [SKLearn libraries](https://scikit-learn.org/stable/). The lather is implemented using the [XGBoost Library](https://xgboost.readthedocs.io/en/stable/python/python_intro.html) with SkLearn and Numpy input support.\n",
    "\n",
    "For this assignment, the [*Rain in Australia*](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package) dataset is used. For more detail, please see the **Data fetching & Data pre-processing** heading. All data is first processed in a Pandas dataframe structure.\n",
    "\n",
    "One note beforehand is on the target attribute: **RainTomorrow**. This attribute is a binary attribute, and the predictions should either be *Rain is predicted* or *No rain is predicted*. In the original data, rougly `78%` of the data is *No rain is predicted*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a6682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility_functions import *\n",
    "\n",
    "## Parameters\n",
    "\n",
    "# Convert numeric values into ranged representations\n",
    "convert_to_range = True\n",
    "# Represent ranges as categorical values or rounded to nearest base value: 12, b=5 -> 10-14\n",
    "range_categorical = False\n",
    "# Convert categorical values (not from ranged values) into one hot representations:\n",
    "# {'smoking': ['sometimes', 'regularly', 'sometimes', 'never']} -> \n",
    "# {'smoking_sometimes': [1, 0, 1, 0], 'smoking_regularly': [0, 1, 0, 0], 'smoking_never': [0, 0, 0, 1]}\n",
    "convert_categorical_to_one_hot = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b470f4a4",
   "metadata": {},
   "source": [
    "## Data fetching & Data pre-processing\n",
    "\n",
    "The [*Rain in Australia*](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package) dataset, is a historical dataset used for classification of rain prediction. The dataset contains `145460` data-entries spread across `21` dimensions, or rather columns/attributes. The data is gathered from around Australia and runs from `2007` up and until `2017`.\n",
    "\n",
    "The data is build up of the following attributes:\n",
    "- **Date** (format yyyy-mm-dd) \\[data_type: string\\] -> requires converting to \\[data_type: datetime64\\]\n",
    "- **Location** \\[data_type: string\\] (categorical) -> requires converting to One Hot representation\n",
    "- **MinTemp** \\[data_type: float64\\]\n",
    "- **MaxTemp** \\[data_type: float64\\]\n",
    "- **Rainfall** \\[data_type: float64\\]\n",
    "- **Evaporation** \\[data_type: float64\\]\n",
    "- **WindGustDir** \\[data_type: string\\] (categorical) -> requires converting to One Hot representation\n",
    "- **WindDir9am** \\[data_type: string\\] (categorical) -> requires converting to One Hot representation\n",
    "- **WindDir3pm** \\[data_type: string\\] (categorical) -> requires converting to One Hot representation\n",
    "- **WindGustSpeed** \\[data_type: float64\\]\n",
    "- **WindSpeed9am** \\[data_type: float64\\]\n",
    "- **WindSpeed3pm** \\[data_type: float64\\]\n",
    "- **Humidity9am** \\[data_type: float64\\]\n",
    "- **Humidity3pm** \\[data_type: float64\\]\n",
    "- **Pressure9am** \\[data_type: float64\\]\n",
    "- **Pressure3pm** \\[data_type: float64\\]\n",
    "- **Cloud9am** \\[data_type: int64\\] (categorical)\n",
    "- **Cloud3pm** \\[data_type: int64\\] (categorical)\n",
    "- **Temp9am** \\[data_type: float64\\]\n",
    "- **Temp3pm** \\[data_type: float64\\]\n",
    "- **RainToday** \\[data_type: string\\] (binary category) -> requires converting to \\[data_type: bool\\]\n",
    "- **RainTomorrow** *TARGET* \\[data_type: string\\] (binary category) -> requires converting to \\[data_type: bool\\]\n",
    "\n",
    "### Data preparation\n",
    "All float values are reduced in their resolution by applying a *nearest range* to them. \n",
    "\n",
    "**Example:**\n",
    "\n",
    "With a range of `range=5`\n",
    "The following array:\n",
    "\n",
    "`[12, 18, 21, 24, 25]`\n",
    "\n",
    "Would be converted to:\n",
    "\n",
    "`[10, 20, 20, 25, 25]`\n",
    "\n",
    "\n",
    "Because of limitation in the used libraries, all categorical (string) attributes are converted into a *one-hot* representation.\n",
    "\n",
    "In this implementation, the choice is made to drop entries with missing data.\n",
    "\n",
    "### Basic attribute selection\n",
    "For all attributes with multiple measurepoints, only the latest point (the 3pm attributes) are used. These are closest to the target event (RainTomorrow).\n",
    "\n",
    "Lastely, data with a lot of entropy, such as the exact dates, and the id of an entry, are left out. The date is however disected into months, to allow for possible seasonality detection.\n",
    "\n",
    "### Advanced attribute selection\n",
    "The exact choice of attributes was partially determined by non-data specific knowledge on [weather forecasting](https://study.com/academy/lesson/what-is-air-pressure-definition-types-causes-effects.html#:~:text=Although%20it%20may%20seem%20like,pressure%2C%20temperature%20and%20air%20density.). However, our primary selection is based on groupings/relations found during the visualisation stages. Please refer to the **observation and discussion** sections within the visualisation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6684b4cf-364a-4197-bc6e-65af5b314691",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>ranged_MinTemp</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>ranged_MaxTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>ranged_Rainfall</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>ranged_Evaporation</th>\n",
       "      <th>...</th>\n",
       "      <th>Location_PerthAirport</th>\n",
       "      <th>Location_Portland</th>\n",
       "      <th>Location_Sale</th>\n",
       "      <th>Location_Sydney</th>\n",
       "      <th>Location_SydneyAirport</th>\n",
       "      <th>Location_Townsville</th>\n",
       "      <th>Location_WaggaWagga</th>\n",
       "      <th>Location_Watsonia</th>\n",
       "      <th>Location_Williamtown</th>\n",
       "      <th>Location_Woomera</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>18</td>\n",
       "      <td>17.9</td>\n",
       "      <td>36</td>\n",
       "      <td>35.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>18</td>\n",
       "      <td>18.4</td>\n",
       "      <td>28</td>\n",
       "      <td>28.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-01-04</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>20</td>\n",
       "      <td>19.4</td>\n",
       "      <td>38</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>22</td>\n",
       "      <td>21.9</td>\n",
       "      <td>38</td>\n",
       "      <td>38.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>Cobar</td>\n",
       "      <td>24</td>\n",
       "      <td>24.2</td>\n",
       "      <td>40</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56415</th>\n",
       "      <td>6</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>Darwin</td>\n",
       "      <td>20</td>\n",
       "      <td>19.3</td>\n",
       "      <td>34</td>\n",
       "      <td>33.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56416</th>\n",
       "      <td>6</td>\n",
       "      <td>2017-06-21</td>\n",
       "      <td>Darwin</td>\n",
       "      <td>22</td>\n",
       "      <td>21.2</td>\n",
       "      <td>32</td>\n",
       "      <td>32.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56417</th>\n",
       "      <td>6</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>Darwin</td>\n",
       "      <td>20</td>\n",
       "      <td>20.7</td>\n",
       "      <td>32</td>\n",
       "      <td>32.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56418</th>\n",
       "      <td>6</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>Darwin</td>\n",
       "      <td>20</td>\n",
       "      <td>19.5</td>\n",
       "      <td>32</td>\n",
       "      <td>31.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56419</th>\n",
       "      <td>6</td>\n",
       "      <td>2017-06-24</td>\n",
       "      <td>Darwin</td>\n",
       "      <td>20</td>\n",
       "      <td>20.2</td>\n",
       "      <td>32</td>\n",
       "      <td>31.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56420 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Month       Date Location  ranged_MinTemp  MinTemp  ranged_MaxTemp  \\\n",
       "0          1 2009-01-01    Cobar              18     17.9              36   \n",
       "1          1 2009-01-02    Cobar              18     18.4              28   \n",
       "2          1 2009-01-04    Cobar              20     19.4              38   \n",
       "3          1 2009-01-05    Cobar              22     21.9              38   \n",
       "4          1 2009-01-06    Cobar              24     24.2              40   \n",
       "...      ...        ...      ...             ...      ...             ...   \n",
       "56415      6 2017-06-20   Darwin              20     19.3              34   \n",
       "56416      6 2017-06-21   Darwin              22     21.2              32   \n",
       "56417      6 2017-06-22   Darwin              20     20.7              32   \n",
       "56418      6 2017-06-23   Darwin              20     19.5              32   \n",
       "56419      6 2017-06-24   Darwin              20     20.2              32   \n",
       "\n",
       "       MaxTemp  ranged_Rainfall  Rainfall  ranged_Evaporation  ...  \\\n",
       "0         35.2                0       0.0                  10  ...   \n",
       "1         28.9                0       0.0                  15  ...   \n",
       "2         37.6                0       0.0                  10  ...   \n",
       "3         38.4                0       0.0                  10  ...   \n",
       "4         41.0                0       0.0                  10  ...   \n",
       "...        ...              ...       ...                 ...  ...   \n",
       "56415     33.4                0       0.0                   5  ...   \n",
       "56416     32.6                0       0.0                  10  ...   \n",
       "56417     32.8                0       0.0                   5  ...   \n",
       "56418     31.8                0       0.0                   5  ...   \n",
       "56419     31.7                0       0.0                   5  ...   \n",
       "\n",
       "       Location_PerthAirport  Location_Portland  Location_Sale  \\\n",
       "0                          0                  0              0   \n",
       "1                          0                  0              0   \n",
       "2                          0                  0              0   \n",
       "3                          0                  0              0   \n",
       "4                          0                  0              0   \n",
       "...                      ...                ...            ...   \n",
       "56415                      0                  0              0   \n",
       "56416                      0                  0              0   \n",
       "56417                      0                  0              0   \n",
       "56418                      0                  0              0   \n",
       "56419                      0                  0              0   \n",
       "\n",
       "      Location_Sydney  Location_SydneyAirport  Location_Townsville  \\\n",
       "0                   0                       0                    0   \n",
       "1                   0                       0                    0   \n",
       "2                   0                       0                    0   \n",
       "3                   0                       0                    0   \n",
       "4                   0                       0                    0   \n",
       "...               ...                     ...                  ...   \n",
       "56415               0                       0                    0   \n",
       "56416               0                       0                    0   \n",
       "56417               0                       0                    0   \n",
       "56418               0                       0                    0   \n",
       "56419               0                       0                    0   \n",
       "\n",
       "      Location_WaggaWagga Location_Watsonia  Location_Williamtown  \\\n",
       "0                       0                 0                     0   \n",
       "1                       0                 0                     0   \n",
       "2                       0                 0                     0   \n",
       "3                       0                 0                     0   \n",
       "4                       0                 0                     0   \n",
       "...                   ...               ...                   ...   \n",
       "56415                   0                 0                     0   \n",
       "56416                   0                 0                     0   \n",
       "56417                   0                 0                     0   \n",
       "56418                   0                 0                     0   \n",
       "56419                   0                 0                     0   \n",
       "\n",
       "       Location_Woomera  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "...                 ...  \n",
       "56415                 0  \n",
       "56416                 0  \n",
       "56417                 0  \n",
       "56418                 0  \n",
       "56419                 0  \n",
       "\n",
       "[56420 rows x 114 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"weatherAUS.csv\"\n",
    "\n",
    "# Fetching CSV and converting to data frame\n",
    "data_file = pd.read_csv(file_path, header=0)\n",
    "\n",
    "# Drop all entries with nan values\n",
    "data_file = data_file.dropna().reset_index(drop=True)\n",
    "\n",
    "# Convert interval and ratio variables into ranges\n",
    "if convert_to_range:\n",
    "    if range_categorical:\n",
    "        r_func = floor_range\n",
    "    else:\n",
    "        r_func = round_to_base\n",
    "    \n",
    "    data_file.insert(data_file.columns.get_loc(\"MinTemp\"), \"ranged_MinTemp\", [r_func(MinTemp, 2) for MinTemp in data_file[\"MinTemp\"]])\n",
    "    data_file.insert(data_file.columns.get_loc(\"MaxTemp\"), \"ranged_MaxTemp\", [r_func(MaxTemp, 2) for MaxTemp in data_file[\"MaxTemp\"]])\n",
    "    data_file.insert(data_file.columns.get_loc(\"Rainfall\"), \"ranged_Rainfall\", [r_func(Rainfall, 2) for Rainfall in data_file[\"Rainfall\"]])\n",
    "    data_file.insert(data_file.columns.get_loc(\"Evaporation\"), \"ranged_Evaporation\", [r_func(Evaporation, 5) for Evaporation in data_file[\"Evaporation\"]])\n",
    "    data_file.insert(data_file.columns.get_loc(\"Sunshine\"), \"ranged_Sunshine\", [r_func(Sunshine, 1) for Sunshine in data_file[\"Sunshine\"]])\n",
    "    data_file.insert(data_file.columns.get_loc(\"WindGustSpeed\"), \"ranged_WindGustSpeed\", [r_func(WindGustSpeed, 5) for WindGustSpeed in data_file[\"WindGustSpeed\"]])\n",
    "    data_file.insert(data_file.columns.get_loc(\"WindSpeed9am\"), \"ranged_WindSpeed9am\", [r_func(WindSpeed9am, 5) for WindSpeed9am in data_file[\"WindSpeed9am\"]])\n",
    "    data_file.insert(data_file.columns.get_loc(\"WindSpeed3pm\"), \"ranged_WindSpeed3pm\", [r_func(WindSpeed3pm, 5) for WindSpeed3pm in data_file[\"WindSpeed3pm\"]])\n",
    "    data_file.insert(data_file.columns.get_loc(\"Humidity9am\"), \"ranged_Humidity9am\", [r_func(Humidity9am, 5) for Humidity9am in data_file[\"Humidity9am\"]])\n",
    "    data_file.insert(data_file.columns.get_loc(\"Humidity3pm\"), \"ranged_Humidity3pm\", [r_func(Humidity3pm, 5) for Humidity3pm in data_file[\"Humidity3pm\"]])\n",
    "    data_file.insert(data_file.columns.get_loc(\"Pressure9am\"), \"ranged_Pressure9am\", [r_func(Pressure9am, 3) for Pressure9am in data_file[\"Pressure9am\"]])\n",
    "    data_file.insert(data_file.columns.get_loc(\"Pressure3pm\"), \"ranged_Pressure3pm\", [r_func(Pressure3pm, 3) for Pressure3pm in data_file[\"Pressure3pm\"]])\n",
    "    data_file.insert(data_file.columns.get_loc(\"Cloud9am\"), \"ranged_Cloud9am\", [r_func(Cloud9am, 3) for Cloud9am in data_file[\"Cloud9am\"]])\n",
    "    data_file.insert(data_file.columns.get_loc(\"Cloud3pm\"), \"ranged_Cloud3pm\", [r_func(Cloud3pm, 3) for Cloud3pm in data_file[\"Cloud3pm\"]])\n",
    "    data_file.insert(data_file.columns.get_loc(\"Temp9am\"), \"ranged_Temp9am\", [r_func(Temp9am, 3) for Temp9am in data_file[\"Temp9am\"]])\n",
    "    data_file.insert(data_file.columns.get_loc(\"Temp3pm\"), \"ranged_Temp3pm\", [r_func(Temp3pm, 3) for Temp3pm in data_file[\"Temp3pm\"]])\n",
    "\n",
    "    \n",
    "# Convert \"boolean\" variables into true boolean variables\n",
    "data_file[\"RainToday\"] = np.where(data_file[\"RainToday\"] == \"Yes\", True, False).astype(\"bool\")\n",
    "data_file[\"RainTomorrow\"] = np.where(data_file[\"RainTomorrow\"] == \"Yes\", True, False).astype(\"bool\")\n",
    "data_file[\"Date\"] = data_file[\"Date\"].astype(\"datetime64\")\n",
    "data_file.insert(data_file.columns.get_loc(\"Date\"), \"Month\", pd.DatetimeIndex(data_file[\"Date\"]).month)\n",
    "    \n",
    "# Convert categorical variables into a one-hot representation\n",
    "if convert_categorical_to_one_hot:\n",
    "    data_file = pd.concat([data_file, pd.get_dummies(data_file[\"WindGustDir\"], prefix=\"WindGustDir\")], axis=1)\n",
    "    data_file = pd.concat([data_file, pd.get_dummies(data_file[\"WindDir9am\"], prefix=\"WindDir9am\")], axis=1)\n",
    "    data_file = pd.concat([data_file, pd.get_dummies(data_file[\"WindDir3pm\"], prefix=\"WindDir3pm\")], axis=1)\n",
    "    data_file = pd.concat([data_file, pd.get_dummies(data_file[\"Location\"], prefix=\"Location\")], axis=1)\n",
    "    \n",
    "data_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c4f40",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3890208",
   "metadata": {},
   "source": [
    "### Test & Train set generation\n",
    "**Applied algorithms for this test and training set**\n",
    "- SKLearn Random Forest Classifier\n",
    "- SKLearn Support Vector Classification Classifier\n",
    "- XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dfa3c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Decision Variables\n",
    "X = data_file.loc[:,[\"RainToday\", \"ranged_Evaporation\", \"ranged_Sunshine\", \"ranged_WindGustSpeed\", \"ranged_Humidity3pm\", \"ranged_Pressure3pm\", \"ranged_Cloud3pm\", \"ranged_Temp3pm\", *fetch_columns_on_name_list(data_file, [\"WindDir9am\", \"WindDir3pm\"])]].values\n",
    "# Target Variable\n",
    "y = data_file.loc[:,\"RainTomorrow\"].values\n",
    "\n",
    "# Split datasets into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Scale data for classifiers\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50115cbb",
   "metadata": {},
   "source": [
    "### Random Forest Classification training & testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5cfa9e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=150, n_jobs=-1)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=150, n_jobs=-1)\n",
    "rf_classifier.fit(X_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d60e95",
   "metadata": {},
   "source": [
    "#### Random Forest Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "63be7f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # Classification report # # #\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.94      0.91      8843\n",
      "        True       0.72      0.53      0.61      2441\n",
      "\n",
      "    accuracy                           0.85     11284\n",
      "   macro avg       0.80      0.74      0.76     11284\n",
      "weighted avg       0.85      0.85      0.85     11284\n",
      "\n",
      "# # # Confusion matrix # # #\n",
      "[[8350  493]\n",
      " [1146 1295]] \n",
      "\n",
      "# # # Accuracy score # # #\n",
      "0.8547500886210564 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "rf_y_prediction = rf_classifier.predict(X_test_scaled)\n",
    "\n",
    "print(\"# # # Classification report # # #\")\n",
    "print(classification_report(Y_test, rf_y_prediction))\n",
    "\n",
    "print(\"# # # Confusion matrix # # #\")\n",
    "print(confusion_matrix(Y_test, rf_y_prediction), \"\\n\")\n",
    "\n",
    "print(\"# # # Accuracy score # # #\")\n",
    "print(accuracy_score(Y_test, rf_y_prediction), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba87e31",
   "metadata": {},
   "source": [
    "#### Random Forest observations and discussions\n",
    "The Random Forest classification technique has fair average of `85%` success, however, this should be viewed critically. Most predictions were made for *No rain* which is also the predominant (`78%`) expected outcome. It therefor made mostly **false negative** mistakes, where it predicted *No rain* eventhough rain was expected (refer to the confusion matrix).\n",
    "\n",
    "Overall, the Random Forest method is most effective at predicting *No rain*, but also shows a fair ability for *expected rain* predictions (`88%` vs `73%`).\n",
    "\n",
    "The Random Forest algorithm has an almost instant training and fitting runtime on this dataset.\n",
    "\n",
    "Because of the fair size of the dataset and number of attributes, the number of estimators has been increased to `150`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936a889",
   "metadata": {},
   "source": [
    "### Support Vector Classification training & testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c787d7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "SV_classifier = SVC()\n",
    "SV_classifier.fit(X_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff741874",
   "metadata": {},
   "source": [
    "#### Support Vector Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e030241d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # Classification report # # #\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.95      0.91      8843\n",
      "        True       0.74      0.49      0.59      2441\n",
      "\n",
      "    accuracy                           0.85     11284\n",
      "   macro avg       0.81      0.72      0.75     11284\n",
      "weighted avg       0.84      0.85      0.84     11284\n",
      "\n",
      "# # # Confusion matrix # # #\n",
      "[[8436  407]\n",
      " [1255 1186]] \n",
      "\n",
      "# # # Accuracy score # # #\n",
      "0.8527118043247075 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "SV_y_prediction = SV_classifier.predict(X_test_scaled)\n",
    "\n",
    "print(\"# # # Classification report # # #\")\n",
    "print(classification_report(Y_test, SV_y_prediction))\n",
    "\n",
    "print(\"# # # Confusion matrix # # #\")\n",
    "print(confusion_matrix(Y_test, SV_y_prediction), \"\\n\")\n",
    "\n",
    "print(\"# # # Accuracy score # # #\")\n",
    "print(accuracy_score(Y_test, SV_y_prediction), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae6084d",
   "metadata": {},
   "source": [
    "#### Support Vector Classification observations and discussions\n",
    "The Support Vector classification technique has fair average of `85%` success, however, this should be viewed critically. Most predictions were made for *No rain* which is also the predominant (`78%`) expected outcome. It therefor made mostly **false negative** mistakes, where it predicted *No rain* eventhough rain was expected (refer to the confusion matrix).\n",
    "\n",
    "Overall, the Support Vector method is most effective at predicting *No rain*, but also shows a fair ability for *expected rain* predictions (`87%` vs `74%`).\n",
    "\n",
    "The Support Vector algorithm has a long runtime both in model building and fitting, requiring several seconds to complete.\n",
    "\n",
    "The support vector alogrithm either didn't seem to change much in results when tuning its parameters, or it got worse. The support vector classifier is therefor left at default settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d9c51",
   "metadata": {},
   "source": [
    "### XGBoost Classification training & testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "482e1c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:56:53] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.6, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=150, n_jobs=16,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "XGB_classifier = xgb.XGBClassifier(base_score=0.6, n_estimators=150)\n",
    "XGB_classifier.fit(X_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bda645",
   "metadata": {},
   "source": [
    "#### XGBoost Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "db2620f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # Classification report # # #\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.94      0.91      8843\n",
      "        True       0.71      0.55      0.62      2441\n",
      "\n",
      "    accuracy                           0.85     11284\n",
      "   macro avg       0.80      0.75      0.77     11284\n",
      "weighted avg       0.85      0.85      0.85     11284\n",
      "\n",
      "# # # Confusion matrix # # #\n",
      "[[8293  550]\n",
      " [1089 1352]] \n",
      "\n",
      "# # # Accuracy score # # #\n",
      "0.8547500886210564 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "XGB_y_prediction = XGB_classifier.predict(X_test_scaled)\n",
    "\n",
    "print(\"# # # Classification report # # #\")\n",
    "print(classification_report(Y_test, XGB_y_prediction))\n",
    "\n",
    "print(\"# # # Confusion matrix # # #\")\n",
    "print(confusion_matrix(Y_test, XGB_y_prediction), \"\\n\")\n",
    "\n",
    "print(\"# # # Accuracy score # # #\")\n",
    "print(accuracy_score(Y_test, XGB_y_prediction), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64da9df1",
   "metadata": {},
   "source": [
    "#### XGBoost Classification observations and discussions\n",
    "The XGBoost classification technique has fair average of `85%` success, however, this should be viewed critically. Most predictions were made for *No rain* which is also the predominant (`78%`) expected outcome. It therefor made mostly **false negative** mistakes, where it predicted *No rain* eventhough rain was expected (refer to the confusion matrix). Although, notably less than the SV and RF classifiers. Instead, it makes more mistakes in **false possitives** compared to the others.\n",
    "\n",
    "Overall, the XGBoost method is most effective at predicting *No rain*, but also shows a fair ability for *expected rain* predictions (`88%` vs `71%`).\n",
    "\n",
    "The XGBoost algorithm has a long runtime in model building, requiring several seconds to complete, however it has an instant fitting routine.\n",
    "\n",
    "Because of the fair size of the dataset and number of attributes, the number of estimators has been increased to `150`. This also seems to be the only attribute to slightly increase accuracy. Otherwise, the classifier is left at default settings for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce39f5e0",
   "metadata": {},
   "source": [
    "### Closing Discussion and Conslusion\n",
    "Overall, the three classification algorithms all seem to have roughly the same *accuracy score*. The **random forest** and **support vector** classifiers both perform almost equal in terms of **false positives/negatives**, however, **random forest** is notably faster than **support vector**.\n",
    "\n",
    "**XGBoost** Seems to make the trade-off with making less **false negative** classification, but in turn has more **false positives** as compared to the other two classification techniques.\n",
    "\n",
    "Looking back at the results of each classifier, they all have fairly similar results. However, one classifier is notably preferable for this dataset and the current settings: **random forest**. This classifier outperforms the other two classifiers by delivering equal results, but with notably shorter runtimes.\n",
    "\n",
    "#### Overfit\n",
    "Looking at the false negatives, all three classifiers seem to show signs of overfitting for *no rain* results. This is not entirely surpising, as most entries are for *no rain*. However, changing parameters to reduce overfitting, such as tree-depth for XGBoost and Random Forest, does not seem to improve results. Since most entries expect a *no rain* result, the argument could be made that there is no case of overfitting at all, after all, it still manages to score over `70%` on the positive cases.\n",
    "\n",
    "#### Hyper parameter tuning\n",
    "The tuning of parameters was mostly performed manually, by hand, in a trial and error fashion. A possible better solution would have been to apply a GRG-Nonlinear parameter optimization search to the parameters of the classifiers, but due to the long runtime of the classifiers, applying this method of optimization would take many hours if not days for one run. Instead, we focussed mostly on tuning the attributes by pre-processing them, and selecting them by first looking at (overlapping) groups in the visualisation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4882ad3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
